# This is the main configuration file for the application.
# ~~~~~

# Secret key
# ~~~~~
# The secret key is used to secure cryptographics functions.
# If you deploy your application to several instances be sure to use the same key!
application.secret="%APPLICATION_SECRET%"

# The application languages
# ~~~~~
application.langs="en"

# Global object class
# ~~~~~
# Define the Global object class for this application.
# Default to Global in the root package.
# application.global=Global

# Router
# ~~~~~
# Define the Router object to use for this application.
# This router will be looked up first when the application is starting up,
# so make sure this is the entry point.
# Furthermore, it's assumed your route file is named properly.
# So for an application router like `my.application.Router`,
# you may need to define a router file `conf/my.application.routes`.
# Default to Routes in the root package (and conf/routes)
# application.router=my.application.Routes

# Database configuration
# ~~~~~
# You can declare as many datasources as you want.
# By convention, the default datasource is named `default`
#
# db.default.driver=org.h2.Driver
# db.default.url="jdbc:h2:mem:play"
# db.default.user=sa
# db.default.password=""

db.default.driver=com.mysql.jdbc.Driver
db.default.url="jdbc:mysql://adm1ss.prod.mediav.com:3306/hadoop"
db.default.user="root"
db.default.password="root"

# Evolutions
# ~~~~~
# You can disable evolutions if needed
evolutionplugin=disabled
applyEvolutions.default=true

# Ebean configuration
# ~~~~~
# You can declare as many Ebean servers as you want.
# By convention, the default server is named `default`
#
ebean.default="models.*"

# Logger
# ~~~~~
# You can also configure logback (http://logback.qos.ch/),
# by providing an application-logger.xml file in the conf directory.

# Root logger:
logger.root=ERROR

# Logger used by the framework:
logger.play=INFO

# Logger provided to your application:
logger.application=DEBUG

hadoop.hdfs.namenode.http.address="nn1ss.prod.mediav.com:50070"
hadoop.mapreduce.historyserver.http.address="rm3ss.prod.mediav.com:19888"
hadoop.yarn.resourcemanager.http.address="rm1ss.prod.mediav.com:8088"
hadoop.hcatalog.webhcat.http.address="rm3ss.prod.mediav.com:50111"

sparksql.thriftserver.jdbc.url="jdbc:hive2://adm1ss.prod.mediav.com:10002/mvdw"
sparksql.thriftserver.jdbc.user="hadoop"
sparksql.thriftserver.jdbc.password=""
sparksql.webui.url=""

hive.thriftserver.jdbc.url="jdbc:hive2://adm1ss.prod.mediav.com:10000/mvdw"
hive.thriftserver.jdbc.user="hadoop"
hive.thriftserver.jdbc.password=""
